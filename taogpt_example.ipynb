{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:06.040583Z",
     "iopub.status.busy": "2023-07-31T06:11:06.040283Z",
     "iopub.status.idle": "2023-07-31T06:11:08.236735Z",
     "shell.execute_reply": "2023-07-31T06:11:08.235796Z",
     "shell.execute_reply.started": "2023-07-31T06:11:06.040560Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-10T04:06:51.742836Z",
     "start_time": "2023-10-10T04:06:50.617380Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "import os\n",
    "\n",
    "from taogpt.orchestrator import *\n",
    "from taogpt.utils import *\n",
    "from taogpt.llm_model import LangChainLLM\n",
    "from taogpt.prompts import PromptDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T04:06:52.354271Z",
     "start_time": "2023-10-10T04:06:52.334742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:08.238613Z",
     "iopub.status.busy": "2023-07-31T06:11:08.238227Z",
     "iopub.status.idle": "2023-07-31T06:11:08.268068Z",
     "shell.execute_reply": "2023-07-31T06:11:08.267171Z",
     "shell.execute_reply.started": "2023-07-31T06:11:08.238592Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-10T04:06:53.312224Z",
     "start_time": "2023-10-10T04:06:53.296003Z"
    }
   },
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:08.269654Z",
     "iopub.status.busy": "2023-07-31T06:11:08.269017Z",
     "iopub.status.idle": "2023-07-31T06:11:10.109116Z",
     "shell.execute_reply": "2023-07-31T06:11:10.108307Z",
     "shell.execute_reply.started": "2023-07-31T06:11:08.269630Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-10T04:06:54.937028Z",
     "start_time": "2023-10-10T04:06:53.796744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"I am using OpenAI's GPT-3 model, which is one of the most advanced language models available. Specifically, I am running on version 3.5.0 of the GPT-3 model. This version was released on March 1, 2023. Is there anything specific you would like to know about my model version?\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(os.environ['HOME'], '.ssh', 'openai-zillow.json'), 'r') as f:\n",
    "    credentials = json.load(f)\n",
    "os.environ[\"OPENAI_API_KEY\"] = credentials['key']\n",
    "os.environ[\"OPENAI_API_BASE\"] = credentials['url']\n",
    "llm3_5 = ChatOpenAI(model_name='gpt-3.5-turbo-16k', temperature=TEMPERATURE)\n",
    "llm4 = ChatOpenAI(model_name='gpt-4', temperature=TEMPERATURE)\n",
    "llm4_32k = ChatOpenAI(model_name='gpt-4-32k', temperature=TEMPERATURE)\n",
    "\n",
    "conversation = ConversationChain(llm=llm3_5)\n",
    "conversation.predict(input=\"What's your model version?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "('gpt-3.5-turbo-16k', 'gpt-4', 'gpt-4-32k')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm3_5.model_name, llm4.model_name, llm4_32k.model_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T04:06:54.980114Z",
     "start_time": "2023-10-10T04:06:54.937470Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:10.111826Z",
     "iopub.status.busy": "2023-07-31T06:11:10.111407Z",
     "iopub.status.idle": "2023-07-31T06:12:26.695021Z",
     "shell.execute_reply": "2023-07-31T06:12:26.694244Z",
     "shell.execute_reply.started": "2023-07-31T06:11:10.111799Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-10T04:09:33.224700Z",
     "start_time": "2023-10-10T04:08:46.892380Z"
    }
   },
   "outputs": [
    {
     "ename": "Pause",
     "evalue": "Pause after initial solving expansion",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPause\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 33\u001B[0m\n\u001B[1;32m     31\u001B[0m experiment_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexample\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# executor.start(\"\"\"What's x mod (y - z) where x = 10, y = 7, z = 4.\"\"\")\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m \u001B[43mexecutor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;43mSolve this 4x4 Sudoku.\u001B[39;49m\n\u001B[1;32m     35\u001B[0m \n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;43m```text\u001B[39;49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124;43m4 3 _ 1\u001B[39;49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;43m1 _ 4 3\u001B[39;49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;43m2 _ 3 _\u001B[39;49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;43m3 4 _ 2\u001B[39;49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;43m```\u001B[39;49m\n\u001B[1;32m     42\u001B[0m \n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;43m(`_` represents an empty cell.)\u001B[39;49m\n\u001B[1;32m     44\u001B[0m \n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;43mSolve in your head. Do NOT use Python programming.\u001B[39;49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:85\u001B[0m, in \u001B[0;36mOrchestrator.start\u001B[0;34m(self, task)\u001B[0m\n\u001B[1;32m     83\u001B[0m     init_analysis_step \u001B[38;5;241m=\u001B[39m AnalysisStep(root_step, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, ROLE_ORCHESTRATOR)\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain\u001B[38;5;241m.\u001B[39mappend(init_analysis_step)\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_with_backtracking\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:120\u001B[0m, in \u001B[0;36mOrchestrator._execute_with_backtracking\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 120\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Backtrack \u001B[38;5;28;01mas\u001B[39;00m backtrack:\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:164\u001B[0m, in \u001B[0;36mOrchestrator._loop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_loop\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m         new_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chain\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28;01mwhile\u001B[39;00m new_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    166\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain\u001B[38;5;241m.\u001B[39mappend(new_step) \u001B[38;5;66;03m# note: we don't necessary eval the new resulting step\u001B[39;00m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/program.py:53\u001B[0m, in \u001B[0;36mStep.eval\u001B[0;34m(self, executor)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21meval\u001B[39m(\u001B[38;5;28mself\u001B[39m, executor: Executor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Step \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 53\u001B[0m     returned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexecutor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m returned\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/program.py:533\u001B[0m, in \u001B[0;36mExpandableStep.eval_only\u001B[0;34m(self, executor)\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrank_choices(executor, n_existing_choices\u001B[38;5;241m=\u001B[39mold_length)\n\u001B[1;32m    532\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfirst_problem_solving_step \u001B[38;5;129;01mand\u001B[39;00m executor\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpause_after_initial_solving_expansion:\n\u001B[0;32m--> 533\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Pause(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPause after initial solving expansion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchoices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchoices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    535\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Backtrack(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo viable options.\u001B[39m\u001B[38;5;124m'\u001B[39m, blame\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mPause\u001B[0m: Pause after initial solving expansion"
     ]
    }
   ],
   "source": [
    "prompts = PromptDb.load_defaults()\n",
    "logger = MarkdownLogger('logs/taogpt_log.md')\n",
    "config = Config(\n",
    "    ask_user_before_execute_codes=False,\n",
    "    ask_user_questions_in_one_prompt=True,\n",
    "    pause_after_initial_solving_expansion=True,\n",
    "    first_expansion=1,\n",
    "    initial_expansion=2,\n",
    "    max_tokens=10000,\n",
    "    check_final=True\n",
    ")\n",
    "\n",
    "primary_model = 4\n",
    "sage_model = None\n",
    "\n",
    "if primary_model == 4:\n",
    "    primary_model = LangChainLLM(llm4, logger=logger)\n",
    "    sage_llm = primary_model\n",
    "else:\n",
    "    primary_model = LangChainLLM(llm3_5, logger=logger, long_context_llm=llm4_32k, long_context_token_threshold=3000)\n",
    "    sage_model = LangChainLLM(llm4, logger=logger, long_context_llm=llm4_32k, long_context_token_threshold=3000) \\\n",
    "        if sage_model is None else primary_model\n",
    "executor = Orchestrator(\n",
    "    config=config,\n",
    "    llm=primary_model,\n",
    "    prompts=prompts,\n",
    "    markdown_logger=logger,\n",
    "    sage_llm=sage_model,\n",
    ")\n",
    "\n",
    "experiment_name = 'example'\n",
    "# executor.start(\"\"\"What's x mod (y - z) where x = 10, y = 7, z = 4.\"\"\")\n",
    "executor.start(\"\"\"\n",
    "Solve this 4x4 Sudoku.\n",
    "\n",
    "```text\n",
    "4 3 _ 1\n",
    "1 _ 4 3\n",
    "2 _ 3 _\n",
    "3 4 _ 2\n",
    "```\n",
    "\n",
    "(`_` represents an empty cell.)\n",
    "\n",
    "Solve in your head. Do NOT use Python programming.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# executor._prompts = PromptDb.load_defaults()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T04:10:26.058066Z",
     "start_time": "2023-10-10T04:10:25.985984Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# executor._chain = executor._chain[:-1]\n",
    "# executor._chain[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T04:10:26.480844Z",
     "start_time": "2023-10-10T04:10:26.452298Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# executor.chain[-1].description"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T04:10:27.272613Z",
     "start_time": "2023-10-10T04:10:27.253651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "executor.resume(10000, additional_tokens_for_smarter_llm=0, unblock_initial_expansion=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:12:26.696369Z",
     "iopub.status.busy": "2023-07-31T06:12:26.696162Z",
     "iopub.status.idle": "2023-07-31T06:12:26.726978Z",
     "shell.execute_reply": "2023-07-31T06:12:26.726366Z",
     "shell.execute_reply.started": "2023-07-31T06:12:26.696352Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-10T04:15:52.199556Z",
     "start_time": "2023-10-10T04:15:52.153718Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = MarkdownLogger(f'examples/{experiment_name}.final.md')\n",
    "executor.log_configs(logger)\n",
    "logger.log_conversation(executor.show_conversation_thread(with_header=True))\n",
    "logger.log(f\"**total tokens**: {executor.llm.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
