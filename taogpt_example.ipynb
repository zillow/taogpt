{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:06.040583Z",
     "iopub.status.busy": "2023-07-31T06:11:06.040283Z",
     "iopub.status.idle": "2023-07-31T06:11:08.236735Z",
     "shell.execute_reply": "2023-07-31T06:11:08.235796Z",
     "shell.execute_reply.started": "2023-07-31T06:11:06.040560Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-12T22:30:51.500931Z",
     "start_time": "2023-10-12T22:30:49.342917Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "import os\n",
    "\n",
    "from taogpt.orchestrator import *\n",
    "from taogpt.utils import *\n",
    "from taogpt.llm_model import LangChainLLM\n",
    "from taogpt.prompts import PromptDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:30:51.569161Z",
     "start_time": "2023-10-12T22:30:51.502471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:08.238613Z",
     "iopub.status.busy": "2023-07-31T06:11:08.238227Z",
     "iopub.status.idle": "2023-07-31T06:11:08.268068Z",
     "shell.execute_reply": "2023-07-31T06:11:08.267171Z",
     "shell.execute_reply.started": "2023-07-31T06:11:08.238592Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-12T22:30:51.570634Z",
     "start_time": "2023-10-12T22:30:51.522624Z"
    }
   },
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:08.269654Z",
     "iopub.status.busy": "2023-07-31T06:11:08.269017Z",
     "iopub.status.idle": "2023-07-31T06:11:10.109116Z",
     "shell.execute_reply": "2023-07-31T06:11:10.108307Z",
     "shell.execute_reply.started": "2023-07-31T06:11:08.269630Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-12T22:30:53.113140Z",
     "start_time": "2023-10-12T22:30:51.540597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'I am currently running on GPT-3.5-turbo, which is the latest version of the OpenAI language model. It was released in June 2021 and is designed to generate more accurate and coherent responses compared to previous versions. Is there anything specific you would like to know about my model version?'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(os.environ['HOME'], '.ssh', 'openai-zillow.json'), 'r') as f:\n",
    "    credentials = json.load(f)\n",
    "os.environ[\"OPENAI_API_KEY\"] = credentials['key']\n",
    "os.environ[\"OPENAI_API_BASE\"] = credentials['url']\n",
    "llm3_5 = ChatOpenAI(model_name='gpt-3.5-turbo-16k', temperature=TEMPERATURE)\n",
    "llm4 = ChatOpenAI(model_name='gpt-4', temperature=TEMPERATURE)\n",
    "llm4_32k = ChatOpenAI(model_name='gpt-4-32k', temperature=TEMPERATURE)\n",
    "\n",
    "conversation = ConversationChain(llm=llm3_5)\n",
    "conversation.predict(input=\"What's your model version?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "('gpt-3.5-turbo-16k', 'gpt-4', 'gpt-4-32k')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm3_5.model_name, llm4.model_name, llm4_32k.model_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:30:53.162629Z",
     "start_time": "2023-10-12T22:30:53.094584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:10.111826Z",
     "iopub.status.busy": "2023-07-31T06:11:10.111407Z",
     "iopub.status.idle": "2023-07-31T06:12:26.695021Z",
     "shell.execute_reply": "2023-07-31T06:12:26.694244Z",
     "shell.execute_reply.started": "2023-07-31T06:11:10.111799Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-12T22:31:14.924673Z",
     "start_time": "2023-10-12T22:30:53.541542Z"
    }
   },
   "outputs": [
    {
     "ename": "Pause",
     "evalue": "Pause after initial solving expansion",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPause\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 37\u001B[0m\n\u001B[1;32m     35\u001B[0m experiment_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexample\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# executor.start(\"\"\"What's x mod (y - z) where x = 10, y = 7, z = 4.\"\"\")\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[43mexecutor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;43mSolve this 4x4 Sudoku.\u001B[39;49m\n\u001B[1;32m     39\u001B[0m \n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;43m```text\u001B[39;49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;43m4 3 _ 1\u001B[39;49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;43m1 _ 4 3\u001B[39;49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;43m3 _ _ 2\u001B[39;49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;43m2 _ 3 _\u001B[39;49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;43m```\u001B[39;49m\n\u001B[1;32m     46\u001B[0m \n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;43m(Solve it in your head, do not write codes)\u001B[39;49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:94\u001B[0m, in \u001B[0;36mOrchestrator.start\u001B[0;34m(self, task)\u001B[0m\n\u001B[1;32m     92\u001B[0m     init_analysis_step \u001B[38;5;241m=\u001B[39m AnalysisStep(root_step, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, ROLE_ORCHESTRATOR)\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain\u001B[38;5;241m.\u001B[39mappend(init_analysis_step)\n\u001B[0;32m---> 94\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_with_backtracking\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:129\u001B[0m, in \u001B[0;36mOrchestrator._execute_with_backtracking\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 129\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Backtrack \u001B[38;5;28;01mas\u001B[39;00m backtrack:\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:178\u001B[0m, in \u001B[0;36mOrchestrator._loop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    176\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_final_solution(last_step)\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 178\u001B[0m new_step \u001B[38;5;241m=\u001B[39m \u001B[43mlast_step\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m new_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain\u001B[38;5;241m.\u001B[39mappend(new_step) \u001B[38;5;66;03m# note: we don't necessary eval the new resulting step\u001B[39;00m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/program.py:74\u001B[0m, in \u001B[0;36mStep.eval\u001B[0;34m(self, executor)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21meval\u001B[39m(\u001B[38;5;28mself\u001B[39m, executor: Executor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Step \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 74\u001B[0m     returned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexecutor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m returned\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/program.py:548\u001B[0m, in \u001B[0;36mExpandableStep.eval_only\u001B[0;34m(self, executor)\u001B[0m\n\u001B[1;32m    546\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrank_choices(executor, n_existing_choices\u001B[38;5;241m=\u001B[39mold_length)\n\u001B[1;32m    547\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfirst_problem_solving_step \u001B[38;5;129;01mand\u001B[39;00m executor\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpause_after_initial_solving_expansion:\n\u001B[0;32m--> 548\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Pause(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPause after initial solving expansion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchoices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchoices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Backtrack(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo viable options.\u001B[39m\u001B[38;5;124m'\u001B[39m, blame\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mPause\u001B[0m: Pause after initial solving expansion"
     ]
    }
   ],
   "source": [
    "prompts = PromptDb.load_defaults()\n",
    "logger = MarkdownLogger('logs/taogpt_log.md')\n",
    "config = Config(\n",
    "    ask_user_before_execute_codes=False,\n",
    "    ask_user_questions_in_one_prompt=True,\n",
    "    pause_after_initial_solving_expansion=True,\n",
    "    first_expansion=1,\n",
    "    initial_expansion=1,\n",
    "    max_search_expansion=2,\n",
    "    first_try_temperature=0.,\n",
    "    max_tokens=10000,\n",
    "    check_final=True\n",
    ")\n",
    "\n",
    "primary_model = 4\n",
    "sage_model = None\n",
    "\n",
    "if primary_model == 4:\n",
    "    primary_model = LangChainLLM(llm4, logger=logger, long_context_llm=llm4_32k, long_context_token_threshold=3000)\n",
    "    _ = sage_model\n",
    "    sage_model = primary_model\n",
    "else:\n",
    "    primary_model = LangChainLLM(llm3_5, logger=logger)\n",
    "    sage_model = LangChainLLM(llm4, logger=logger, long_context_llm=llm4_32k, long_context_token_threshold=3000) \\\n",
    "        if sage_model is None or sage_model == 4 else primary_model\n",
    "\n",
    "executor = Orchestrator(\n",
    "    config=config,\n",
    "    llm=primary_model,\n",
    "    prompts=prompts,\n",
    "    markdown_logger=logger,\n",
    "    sage_llm=sage_model,\n",
    ")\n",
    "\n",
    "experiment_name = 'example'\n",
    "# executor.start(\"\"\"What's x mod (y - z) where x = 10, y = 7, z = 4.\"\"\")\n",
    "executor.start(\"\"\"\n",
    "Solve this 4x4 Sudoku.\n",
    "\n",
    "```text\n",
    "4 3 _ 1\n",
    "1 _ 4 3\n",
    "3 _ _ 2\n",
    "2 _ 3 _\n",
    "```\n",
    "\n",
    "(Solve it in your head, do not write codes)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "backup_chain = executor.chain.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:41:26.575988Z",
     "start_time": "2023-10-12T22:41:26.534542Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# executor._prompts = PromptDb.load_defaults()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T19:31:27.733062Z",
     "start_time": "2023-10-12T19:31:27.690090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "\"'[at step: Fill in the missing numbers in the third row]\\n\\nThe third row of the Sudoku puzzle is `3 _ _ 2`. The missing numbers in this row are `1` and `4`, because each row in a Sudoku puzzle must contain all of the digits from 1 to the grid size exactly once, and `1` and `4` are the only digits from 1 to 4 that are not already in the row.\\n\\nSo, the completed third row is `3 1 4 2`.'\""
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor._chain = executor._chain[:10]\n",
    "executor._chain[-1].description"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:44:07.826816Z",
     "start_time": "2023-10-12T22:44:07.790157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "executor.resume(10000, additional_tokens_for_smarter_llm=500, unblock_initial_expansion=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:46:33.974355Z",
     "start_time": "2023-10-12T22:44:52.115931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:12:26.696369Z",
     "iopub.status.busy": "2023-07-31T06:12:26.696162Z",
     "iopub.status.idle": "2023-07-31T06:12:26.726978Z",
     "shell.execute_reply": "2023-07-31T06:12:26.726366Z",
     "shell.execute_reply.started": "2023-07-31T06:12:26.696352Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-12T23:12:42.738775Z",
     "start_time": "2023-10-12T23:12:42.693279Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = MarkdownLogger(f'examples/{experiment_name}.final.md')\n",
    "executor.log_configs(logger)\n",
    "logger.log_conversation(executor.show_conversation_thread(with_header=True))\n",
    "logger.log(f\"**total tokens**: {executor.llm.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
