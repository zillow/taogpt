{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:06.040583Z",
     "iopub.status.busy": "2023-07-31T06:11:06.040283Z",
     "iopub.status.idle": "2023-07-31T06:11:08.236735Z",
     "shell.execute_reply": "2023-07-31T06:11:08.235796Z",
     "shell.execute_reply.started": "2023-07-31T06:11:06.040560Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-01T22:43:33.633456Z",
     "start_time": "2023-10-01T22:43:32.349201Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "import os\n",
    "\n",
    "from taogpt.orchestrator import *\n",
    "from taogpt.utils import *\n",
    "from taogpt.llm_model import LangChainLLM\n",
    "from taogpt.prompts import PromptDb\n",
    "import taogpt.utils as utils\n",
    "\n",
    "utils.enable_debugging(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T22:43:33.651516Z",
     "start_time": "2023-10-01T22:43:33.633973Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:08.238613Z",
     "iopub.status.busy": "2023-07-31T06:11:08.238227Z",
     "iopub.status.idle": "2023-07-31T06:11:08.268068Z",
     "shell.execute_reply": "2023-07-31T06:11:08.267171Z",
     "shell.execute_reply.started": "2023-07-31T06:11:08.238592Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-01T22:43:34.212580Z",
     "start_time": "2023-10-01T22:43:34.190461Z"
    }
   },
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:08.269654Z",
     "iopub.status.busy": "2023-07-31T06:11:08.269017Z",
     "iopub.status.idle": "2023-07-31T06:11:10.109116Z",
     "shell.execute_reply": "2023-07-31T06:11:10.108307Z",
     "shell.execute_reply.started": "2023-07-31T06:11:08.269630Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-01T22:43:35.574048Z",
     "start_time": "2023-10-01T22:43:34.877420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"I am using OpenAI's GPT-3 model, specifically version 3.0.6.\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(os.environ['HOME'], '.ssh', 'openai-zillow.json'), 'r') as f:\n",
    "    credentials = json.load(f)\n",
    "os.environ[\"OPENAI_API_KEY\"] = credentials['key']\n",
    "os.environ[\"OPENAI_API_BASE\"] = credentials['url']\n",
    "llm3_5 = ChatOpenAI(model_name='gpt-3.5-turbo-16k', temperature=TEMPERATURE)\n",
    "llm4 = ChatOpenAI(model_name='gpt-4-32k', temperature=TEMPERATURE)\n",
    "\n",
    "conversation = ConversationChain(llm=llm3_5)\n",
    "conversation.predict(input=\"What's your model version?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "('gpt-3.5-turbo-16k', 'gpt-4-32k')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm3_5.model_name, llm4.model_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T22:43:36.513788Z",
     "start_time": "2023-10-01T22:43:36.477809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:11:10.111826Z",
     "iopub.status.busy": "2023-07-31T06:11:10.111407Z",
     "iopub.status.idle": "2023-07-31T06:12:26.695021Z",
     "shell.execute_reply": "2023-07-31T06:12:26.694244Z",
     "shell.execute_reply.started": "2023-07-31T06:11:10.111799Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-03T00:25:43.114863Z",
     "start_time": "2023-10-03T00:19:36.381342Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "User cancelled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 20\u001B[0m\n\u001B[1;32m     11\u001B[0m executor \u001B[38;5;241m=\u001B[39m Orchestrator(\n\u001B[1;32m     12\u001B[0m     config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[1;32m     13\u001B[0m     llm\u001B[38;5;241m=\u001B[39mLangChainLLM(llm4, logger\u001B[38;5;241m=\u001B[39mlogger),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m     sage_llm\u001B[38;5;241m=\u001B[39mLangChainLLM(llm4, logger\u001B[38;5;241m=\u001B[39mlogger),\n\u001B[1;32m     17\u001B[0m )\n\u001B[1;32m     19\u001B[0m experiment_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexample\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 20\u001B[0m \u001B[43mexecutor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;43mGiven a game board of Weiqi, create a Python program to calculate the scores of each player. Use the Chinese rules \u001B[39;49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124;43mfor scoring.\u001B[39;49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43manalyze_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# executor.start(\"\"\"\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Solve this Game of 24: 7, 6, 2, 1\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# (Use each number exactly once. Allowed operations: +, -, *, /, and parentheses.)\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# \"\"\", analyze_first=True)\u001B[39;00m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:83\u001B[0m, in \u001B[0;36mOrchestrator.start\u001B[0;34m(self, task, analyze_first)\u001B[0m\n\u001B[1;32m     81\u001B[0m     init_analysis_step \u001B[38;5;241m=\u001B[39m AnalysisStep(root_step, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, ROLE_ORCHESTRATOR)\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain\u001B[38;5;241m.\u001B[39mappend(Invocation(init_analysis_step, _executor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m))\n\u001B[0;32m---> 83\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_with_backtracking\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:99\u001B[0m, in \u001B[0;36mOrchestrator._execute_with_backtracking\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 99\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_frozen_chain \u001B[38;5;241m=\u001B[39m _FrozenList(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain)\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:161\u001B[0m, in \u001B[0;36mOrchestrator._loop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_token_usages()\n\u001B[0;32m--> 161\u001B[0m     new_step \u001B[38;5;241m=\u001B[39m \u001B[43mnew_step\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_invocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m new_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnext_step()\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/program.py:67\u001B[0m, in \u001B[0;36mStep.eval\u001B[0;34m(self, my_invocation)\u001B[0m\n\u001B[1;32m     65\u001B[0m sig \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mid\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mid\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mid\u001B[39m(my_invocation\u001B[38;5;241m.\u001B[39mstep)\n\u001B[0;32m---> 67\u001B[0m returned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmy_invocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m returned\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/program.py:314\u001B[0m, in \u001B[0;36mAskQuestionStep.eval_only\u001B[0;34m(self, my_invocation)\u001B[0m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_need_consolidate \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    313\u001B[0m questions \u001B[38;5;241m=\u001B[39m parse_ordered_list(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdescription)\n\u001B[0;32m--> 314\u001B[0m reply \u001B[38;5;241m=\u001B[39m \u001B[43mmy_invocation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecutor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_user\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    315\u001B[0m reply \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mstr_or_blank(reply)\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reply \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:340\u001B[0m, in \u001B[0;36mOrchestrator.ask_user\u001B[0;34m(self, questions, input_fn)\u001B[0m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mask_user\u001B[39m(\u001B[38;5;28mself\u001B[39m, questions: [\u001B[38;5;28mstr\u001B[39m], input_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28minput\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;66;03m# for now just use the console input\u001B[39;00m\n\u001B[0;32m--> 340\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mask_questions\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_user_questions_in_one_prompt\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:416\u001B[0m, in \u001B[0;36mask_questions\u001B[0;34m(input_fn, questions, one_prompt)\u001B[0m\n\u001B[1;32m    414\u001B[0m     reply \u001B[38;5;241m=\u001B[39m input_fn(user_prompt)\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m reply\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcancel\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 416\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser cancelled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m reply\n\u001B[1;32m    418\u001B[0m reply \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: User cancelled"
     ]
    }
   ],
   "source": [
    "prompts = PromptDb.load_defaults()\n",
    "logger = MarkdownLogger('logs/taogpt_log.md')\n",
    "config = Config(\n",
    "    ask_user_before_execute_codes=False,\n",
    "    ask_user_questions_in_one_prompt=True,\n",
    "    initial_expansion=2,\n",
    "    max_tree_branches=8,\n",
    "    max_tokens=20000,\n",
    "    check_final=True\n",
    ")\n",
    "executor = Orchestrator(\n",
    "    config=config,\n",
    "    llm=LangChainLLM(llm4, logger=logger),\n",
    "    prompts=prompts,\n",
    "    markdown_logger=logger,\n",
    "    sage_llm=LangChainLLM(llm4, logger=logger),\n",
    ")\n",
    "\n",
    "experiment_name = 'example'\n",
    "executor.start(\"\"\"\n",
    "Given a game board of Weiqi, create a Python program to calculate the scores of each player. Use the Chinese rules\n",
    "for scoring.\n",
    "\"\"\", analyze_first=True)\n",
    "# executor.start(\"\"\"\n",
    "# Solve this Game of 24: 7, 6, 2, 1\n",
    "# (Use each number exactly once. Allowed operations: +, -, *, /, and parentheses.)\n",
    "# \"\"\", analyze_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extend token allowance by 10000 to 40000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Regular LLM consumed 40059 tokens, exceeded allowance of 40000",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mexecutor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresume\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:93\u001B[0m, in \u001B[0;36mOrchestrator.resume\u001B[0;34m(self, additional_tokens, additional_tokens_for_smarter_llm)\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_tokens_for_sage_llm \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m additional_tokens_for_smarter_llm\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mextend token allowance for smarter LLM by \u001B[39m\u001B[38;5;132;01m{\u001B[39;00madditional_tokens_for_smarter_llm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     92\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_tokens_for_sage_llm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 93\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_with_backtracking\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:99\u001B[0m, in \u001B[0;36mOrchestrator._execute_with_backtracking\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 99\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_frozen_chain \u001B[38;5;241m=\u001B[39m _FrozenList(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chain)\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:160\u001B[0m, in \u001B[0;36mOrchestrator._loop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msummarize_final_answer()\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_token_usages\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m     new_step \u001B[38;5;241m=\u001B[39m new_step\u001B[38;5;241m.\u001B[39meval(new_invocation)\n\u001B[1;32m    162\u001B[0m new_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnext_step()\n",
      "File \u001B[0;32m~/ml/taogpt/src/taogpt/orchestrator.py:174\u001B[0m, in \u001B[0;36mOrchestrator.check_token_usages\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_token_usages\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_tokens \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mtotal_tokens:\n\u001B[0;32m--> 174\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRegular LLM consumed \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mtotal_tokens\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokens, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    175\u001B[0m                            \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexceeded allowance of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_tokens\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mid\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mid\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msage_llm) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_tokens_for_sage_llm \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msage_llm\u001B[38;5;241m.\u001B[39mtotal_tokens:\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSmarter LLM consumed \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msage_llm\u001B[38;5;241m.\u001B[39mtotal_tokens\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokens, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    178\u001B[0m                            \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexceeded allowance of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_tokens_for_sage_llm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Regular LLM consumed 40059 tokens, exceeded allowance of 40000"
     ]
    }
   ],
   "source": [
    "executor.resume(10000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T00:11:28.920542Z",
     "start_time": "2023-10-03T00:10:56.536091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-31T06:12:26.696369Z",
     "iopub.status.busy": "2023-07-31T06:12:26.696162Z",
     "iopub.status.idle": "2023-07-31T06:12:26.726978Z",
     "shell.execute_reply": "2023-07-31T06:12:26.726366Z",
     "shell.execute_reply.started": "2023-07-31T06:12:26.696352Z"
    },
    "ExecuteTime": {
     "end_time": "2023-10-01T23:48:49.536396Z",
     "start_time": "2023-10-01T23:48:49.498302Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = MarkdownLogger(f'examples/{experiment_name}.final.md')\n",
    "logger.log_conversation(executor.show_conversation_thread(with_extras=True))\n",
    "logger.log(f\"**total tokens**: {executor.llm.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
